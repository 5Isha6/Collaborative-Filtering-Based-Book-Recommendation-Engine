{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import NormalPredictor\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import NMF,SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.model_selection import cross_validate\n",
    "import pandas as pd\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import Reader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users 52541\n",
      "number of books 152\n"
     ]
    }
   ],
   "source": [
    "r = pd.read_csv('ratings_filtered_data.csv')\n",
    "r=r.drop( 'Unnamed: 0',axis=1);\n",
    "r.shape\n",
    "n_users = len(list(set(r.user_id)))\n",
    "n_books = len(list(set(r.book_id)))\n",
    "print('number of users', n_users)\n",
    "print('number of books', n_books)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As number of users >> number of sparse books should not be a problem for modeling with this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection of Datasize for Modeling\n",
    "\n",
    "As the first step for modeling, lets investigate if we can consider a subset of the available rating dataset to be able to perform all the modeling. This can be explored by increasing number of obversations considered in the dataset at small increments, and then estimating the efficiency of modeling with the increase of the size of the dataset. We will use a simple memory based algorithm (available as KNNWithMeans with the sklearn SURPRISE pakacage) for this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9093\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8992\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8962\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8769\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8836\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8872\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8916\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8835\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8735\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8841\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8921\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8822\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8847\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8839\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8812\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8894\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8907\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8887\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8833\n",
      "Estimating biases using als...\n"
     ]
    }
   ],
   "source": [
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "\n",
    "rmse = [] # Initiate an empty list to store RMSE for every iteration\n",
    "size_considered = [] # Initiate an empty list to store datasize for every iteration\n",
    "\n",
    "for i in range (1,25):\n",
    "    r_selected = r[:i*10000]\n",
    "    size_considered.append(i*10000)\n",
    "    \n",
    "# prepare a dataset object for processing with surprise package .\n",
    "    data_set = Dataset.load_from_df(r_selected,reader)\n",
    "\n",
    "#Split into test and train\n",
    "    train_set, test_set = train_test_split(data_set,test_size =0.2)\n",
    "# User based collaborative filtering\n",
    "    knn = KNNWithMeans(k=50, sim_options={'name': 'pearson_baseline', 'user_based': True})\n",
    "    knn.fit(train_set)\n",
    "    knn_pred = knn.test(test_set) # predict ratings for the testset\n",
    "    rmse.append(accuracy.rmse(knn_pred)) # compute RMSE score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x = size_considered,y = rmse, color = 'g')\n",
    "plt.xlabel('Number of Observations Considered in the Iteration')\n",
    "plt.ylabel('RMSE Score')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.title('Effect of Datasize on Modeling Accuracy (RMSE)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "Since there is not siginificant imporvement of the RMSE with the increase in datasize for modeling, and we can extrapolate that model performance will be similar for predicting large amount of data, and consider the  first 100,000 data for modeling. Lets redefine the dataset with 100,000 observation for all the next steps in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for Rest of the Modeling Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A reader is needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "#Select Dataset with 100,000 observation\n",
    "r_select = r[:100000]\n",
    "# Prepare the dataset for processing with surprise package\n",
    "data_set = Dataset.load_from_df(r_select,reader)\n",
    "\n",
    "#Split into test and train\n",
    "train_set, test_set = train_test_split(data_set,test_size =0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Redo KNNwithMean (this time with 100,000 dataset and crossvalidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# knn was previosuly defined as below \n",
    "knn = KNNWithMeans(k=50, sim_options={'name': 'pearson_baseline', 'user_based': True})\n",
    "# Run 3-fold cross-validation and print results.\n",
    "knn_cv = cross_validate(knn, data_set, measures=['RMSE', 'MAE'], cv=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data in a dictionary to make a final summary at the end\n",
    "Model_Summary = {}\n",
    "Model_Summary ['Model_Name'] = ['KNNWithMeans']\n",
    "Model_Summary ['GridSearch (Y/N)']  = ['N']\n",
    "Model_Summary ['Paramters']  = ['k=50, name: pearson_baseline, user_based: True,min_support = 1}']\n",
    "Model_Summary ['RMSE']  = [knn_cv['test_rmse'].mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Does KNN compare to Baseline ML Model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets Define a BaseLine Model with Surprise \n",
    "nd =  NormalPredictor()\n",
    "\n",
    "# Run 3-fold cross-validation and print results.\n",
    "nd_cv = cross_validate(nd, data_set, measures=['RMSE', 'MAE'], cv=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data in a dictionary to make a final summary at the end\n",
    "\n",
    "Model_Summary ['Model_Name'].append('Normal Predictor')\n",
    "Model_Summary ['GridSearch (Y/N)'].append('N')\n",
    "Model_Summary ['Paramters'].append('-')\n",
    "Model_Summary ['RMSE'].append(nd_cv['test_rmse'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define all parameter options\n",
    "k = [30,40,50]\n",
    "sim_options = {\n",
    "    \"name\": [\"msd\", \"cosine\"],\n",
    "    \"min_support\": [3, 4, 5],\n",
    "    \"user_based\": [False, True],\n",
    "}\n",
    "\n",
    "param_grid = {'k':k, \"sim_options\": sim_options}\n",
    "\n",
    "gs_knn = GridSearchCV(KNNWithMeans, param_grid, measures=[\"rmse\", \"mae\"], cv=3)\n",
    "gs_knn.fit(data_set)\n",
    "\n",
    "print(gs_knn.best_score[\"rmse\"])\n",
    "print(gs_knn.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Summary ['Model_Name'].append('KNNWithMeans')\n",
    "Model_Summary ['GridSearch (Y/N)'].append('Y')\n",
    "Model_Summary ['Paramters'].append(gs_knn.best_params[\"rmse\"])\n",
    "Model_Summary ['RMSE'].append(gs_knn.best_score[\"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD for Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# simple SVD model\n",
    "svd = SVD(n_factors=20, n_epochs = 30, biased=False) # initiate a SVD algorithm object\n",
    "\n",
    "# Run 3-fold cross-validation and print results.\n",
    "svd_cv = cross_validate(svd, data_set, measures=['RMSE', 'MAE'], cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Summary ['Model_Name'].append('SVD')\n",
    "Model_Summary ['GridSearch (Y/N)'].append('N')\n",
    "Model_Summary ['Paramters'].append('n_factors=20, n_epochs = 30, biased=False')\n",
    "Model_Summary ['RMSE'].append(svd_cv['test_rmse'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter optimization with scikit-surprise SVD algorithm with bias\n",
    "\n",
    "# cross validation to optimize parameters of SVD with bias\n",
    "param_grid = {'n_factors': [10,20,30,50], 'n_epochs': [50,100,200], 'lr_all': [0.005],'reg_all': [0.05], 'biased': [True]}\n",
    "gs_svd = GridSearchCV(SVD, param_grid, measures=['rmse'], cv = 3)\n",
    "gs_svd.fit(data_set) # gridsesarch optimization on the trainset\n",
    "\n",
    "# best RMSE score\n",
    "print(gs_svd.best_score)\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs_svd.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Summary ['Model_Name'].append('SVD')\n",
    "Model_Summary ['GridSearch (Y/N)'].append('Y')\n",
    "Model_Summary ['Paramters'].append(gs_svd.best_params)\n",
    "Model_Summary ['RMSE'].append(gs_svd.best_score['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_svd.best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# simple SVD model\n",
    "nmf = NMF(n_factors=20, n_epochs = 30, biased = True) # initiate a SVD algorithm object\n",
    "# Run 3-fold cross-validation and print results.\n",
    "nmf_cv = cross_validate(nmf, data_set, measures=['RMSE', 'MAE'], cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Summary ['Model_Name'].append(' NMF')\n",
    "Model_Summary ['GridSearch (Y/N)'].append ('N')\n",
    "Model_Summary ['Paramters'].append('n_factors=20, n_epochs = 30, biased = True')\n",
    "Model_Summary ['RMSE'].append(nmf_cv['test_rmse'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search with NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import NMF\n",
    "# cross validation to optimize parameters of NMF with no bias\n",
    "param_grid = {'n_factors': [10,20,30,50], 'n_epochs': [20, 30,40,50],'biased': [False, True]}\n",
    "gs_nmf = GridSearchCV(NMF, param_grid, measures=['rmse'], cv=3)\n",
    "gs_nmf.fit(data_set) # gridsesarch optimization on the trainset, need to feed in a Dataset object not a trainset object\n",
    "\n",
    "# best RMSE score\n",
    "print(gs_nmf.best_score)\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs_nmf.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Summary ['Model_Name'].append('NMF')\n",
    "Model_Summary ['GridSearch (Y/N)'].append('Y')\n",
    "Model_Summary ['Paramters'].append(gs_nmf.best_params[\"rmse\"])\n",
    "Model_Summary ['RMSE'].append(gs_nmf.best_score[\"rmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# update settings to display untruncated dataframe\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Show Modeling Summary\n",
    "CF_Modeling_Summary = pd.DataFrame.from_dict(Model_Summary)\n",
    "CF_Modeling_Summary.sort_values(by = 'RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets apply the best performing model on unseen data and compare results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets predict on another set of  data with best performing model \n",
    "hold_data = r[100000:200000]\n",
    "\n",
    "# A reader is needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Prepare the dataset for processing with surprise package\n",
    "data_set = Dataset.load_from_df(hold_data,reader)\n",
    "\n",
    "#Split the data to train and test\n",
    "train , test = train_test_split(data_set, test_size = 0.5)\n",
    "\n",
    "\n",
    "# define SVD model with best paramters \n",
    "svd = SVD(n_factors = 30, n_epochs = 50, lr_all =  0.005, reg_all = 0.05, biased = True)\n",
    "svd.fit(train)\n",
    "test_pred = svd.test(test)\n",
    "accuracy.rmse(test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round up Predictions and compare  Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comparison = []\n",
    "actual_rating = []\n",
    "\n",
    "for user, item, rating in test:\n",
    "    actual_rating.append(rating)\n",
    "    \n",
    "#plt.hist(actual_rating, color = 'g')\n",
    "\n",
    "\n",
    "for item in test_pred:\n",
    "    comparison.append((item[3]))\n",
    "#plt.hist(comparison,color = 'b')\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "dataset ['actual_rating'] =actual_rating\n",
    "dataset ['predicted_rating'] = (comparison)\n",
    "dataset [['actual_rating','predicted_rating']].plot(kind='hist',bins=[0, 1, 2, 3, 4, 5], alpha=0.5) \n",
    "plt.xlabel('Rating')\n",
    "plt.title('Comparison of Histograms: Actual Rating vs Predicted Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting Prediction Results to a DataFrame \n",
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the Prediction Results to a DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = {'user_id': [], 'book_id': [],'Predicted Rating': [] }\n",
    "for element in test_pred:\n",
    "    prediction['user_id'].append(element.uid)\n",
    "    prediction['book_id'].append(element.iid)\n",
    "    prediction['Predicted Rating'].append(element.est)\n",
    "#prediction\n",
    "prediction_dataframe = pd.DataFrame.from_dict(prediction)  \n",
    "prediction_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the dataset\n",
    "prediction_dataframe.to_csv('Rating_Prediction.csv', encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
